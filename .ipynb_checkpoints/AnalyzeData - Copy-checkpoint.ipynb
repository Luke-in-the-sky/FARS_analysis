{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and modules\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FARS_data_files = {2014 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2014.txt',\n",
    "                   #2013 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2013.txt',\n",
    "                   #2012 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2012.txt',\n",
    "                   #2011 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2011.txt',\n",
    "                   #2010 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2010.txt'\n",
    "                  }\n",
    "\n",
    "def size_of_dataframe(df):\n",
    "    if type(df)==pd.core.series.Series or type(df)==pd.sparse.series.SparseSeries:\n",
    "            return (df.values.nbytes + df.index.nbytes)\n",
    "    return (df.values.nbytes + df.index.nbytes + df.columns.nbytes)\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "# ------------------\n",
    "\n",
    "# Check if files exists\n",
    "def load_file(file_):\n",
    "    print('Loading', file_, '...')\n",
    "    try:\n",
    "        df = pd.read_csv(file_, engine = 'python', sep='\\t', index_col='Obs.')\n",
    "    except OSError:\n",
    "        print('!!file not found!!', file_)\n",
    "        return np.nan\n",
    "    return df\n",
    "\n",
    "def build_dataset():\n",
    "    FARS_dataframes = {}\n",
    "    for year in FARS_data_files.keys():\n",
    "        FARS_dataframes[year] = load_file(FARS_data_files[year])\n",
    "\n",
    "    # Put them all in one dataframe\n",
    "    data = pd.concat(FARS_dataframes)#, ignore_index = True)\n",
    "\n",
    "    # Eliminate the 'Unnamed' column we got from the reading of the csv\n",
    "    for key in data.keys():\n",
    "        if key.startswith('Unnamed'):\n",
    "            data = data.drop(key, 1)\n",
    "    print('Number of rows in dataset:',len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select interesting stuff\n",
    "# --------------\n",
    "\n",
    "# Load Code manual\n",
    "import pickle\n",
    "code_manual = pickle.load( open( \"CodeManual.p\", \"rb\" ) )\n",
    "\n",
    "# Select interesting features and map them to their respective\n",
    "#   names in the code manual\n",
    "translate_column_names = {\n",
    "#'driverdrowsy' : 'Drowsy driver',\n",
    "#'arrtime' : 'Arrival time EMS',\n",
    "'arrhr'   : 'Arrival hour EMS',\n",
    "'accdate' : 'Crash Date (mmddyyyy)',\n",
    "'acchr'   : 'Crash Hour',\n",
    "'alcres'  : 'Alcohol test results',  #Knowledge of alcres is mostly unknown and biased towards fatalities\n",
    "#'manncol' : 'Manner of Collision',\n",
    "'deathdate' : 'Death Date',\n",
    "#'injury'  : 'Injury severity',\n",
    "'ptype'   : 'Person Type',\n",
    "'druginv' : 'Police Reported Drug Involvement',\n",
    "#'heavytruck': 'Large Truck Related',\n",
    "#'schlbus' : 'School Bus Related',\n",
    "'sex' : 'Sex',\n",
    "#'race' : 'Race',\n",
    "#'reljuncinter' : 'Relation To Junction: Within Interchange Area',\n",
    "'atmcond': 'Atmospheric Condition (1)',\n",
    "#'holiday' : 'Holiday Related',\n",
    "#'nhs' : 'National Highway System',\n",
    "#'hispanic' : 'Hispanic',\n",
    "'rfun' : 'Roadway Function Class',\n",
    "'lightcond' : 'Light Condition',\n",
    "'speeding' : 'Speeding',\n",
    "'dayofweek' : 'Day Of Week',\n",
    "#'latitude' : 'Latitude',\n",
    "#'longitude': 'Longitude'\n",
    "}\n",
    "\n",
    "also_interesting = ['age']\n",
    "\n",
    "labelsOfInter = list(translate_column_names.keys())\n",
    "labelsOfInter.extend(also_interesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collapse some features values\n",
    "# ------------\n",
    "\n",
    "# Categorize on number of Fatalities\n",
    "def NumFatalitiesToCategory(num):\n",
    "    if num <3:\n",
    "        return 1\n",
    "    elif (num>=3) and (num<6):\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "# Reduce categories in 'Light Condition'\n",
    "code_manual[translate_column_names['lightcond']][11] = 'Dawn/Dusk'\n",
    "code_manual[translate_column_names['lightcond']][12] = 'Dark'\n",
    "def Collaps_lightcond(num):\n",
    "    if num == 4 or num == 5:\n",
    "        return 11\n",
    "    elif num in [6,8,9]: #unknown\n",
    "        return -1\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"Holiday Related\"\n",
    "if 'holiday' in labelsOfInter:\n",
    "    code_manual[translate_column_names['holiday']][-1] = 'Not Holiday or Unknow date'\n",
    "    code_manual[translate_column_names['holiday']][1] = 'Was Holiday'\n",
    "def Collaps_holiday(num):\n",
    "    if num >0:\n",
    "        return 1\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"Atmospheric Condition (1)\"\n",
    "code_manual[translate_column_names['atmcond']][1] = 'Clear or Cloudy'\n",
    "code_manual[translate_column_names['atmcond']][2] = 'Precipitation'\n",
    "code_manual[translate_column_names['atmcond']][4] = 'Snow'\n",
    "code_manual[translate_column_names['atmcond']][6] = \"Severe \\\n",
    "Crosswinds, Blowing Sand, Soil, Dirt\",\n",
    "def Collaps_atmcond(num):\n",
    "    if num in [8, 0, 98, 99]: #other or not known\n",
    "        return -1\n",
    "    elif num in [1,10]:\n",
    "        return 1\n",
    "    elif num in [2,12,3]:\n",
    "        return 2\n",
    "    elif num in [6,7]:\n",
    "        return 6\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"Hispanic\"\n",
    "if 'hispanic' in labelsOfInter:\n",
    "    code_manual[translate_column_names['hispanic']][10] = 'Is Hispanic'\n",
    "    def Collaps_hispanic(num):\n",
    "        if num in [-1,99,0]: #other or not known\n",
    "            return -1\n",
    "        elif num in range(1,7):\n",
    "            return 10\n",
    "        return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"Race\"\n",
    "if 'race' in labelsOfInter:\n",
    "    code_manual[translate_column_names['race']][30] = 'American Indian or Hawaiian'\n",
    "    code_manual[translate_column_names['race']][31] = 'Asian (not Indian)'\n",
    "    code_manual[translate_column_names['race']][32] = 'Indian'\n",
    "    def Collaps_race(num):\n",
    "        if num in [-1, 0, 98, 99, 97]: #other or not known\n",
    "            return -1\n",
    "        elif num in [1,2]:\n",
    "            return num+27\n",
    "        elif num in [3,6]:  \n",
    "            return 30\n",
    "        elif num in [4,5,7,28,38,48,58,68,78]:  \n",
    "            return 31\n",
    "        elif num in [18,19]:  \n",
    "            return 32\n",
    "        return num\n",
    "\n",
    "# Map sex=1 ('male') to sex=0 (for SparseDataFrame efficiency)\n",
    "code_manual[translate_column_names['sex']][0] = 'Male'\n",
    "def Collaps_sex(num):\n",
    "    if num==1:\n",
    "        return 0\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"Roadway Function Class\"\n",
    "code_manual[translate_column_names['rfun']][21] = 'Rural-Arterial road'\n",
    "code_manual[translate_column_names['rfun']][22] = 'Rural-Collector/Local road'\n",
    "code_manual[translate_column_names['rfun']][24] = 'Urban-Arterial road'\n",
    "code_manual[translate_column_names['rfun']][25] = 'Urban-Collector/Local road'\n",
    "def Collaps_rfun(num):\n",
    "    if num in [1,2,3]:\n",
    "        return 21\n",
    "    elif num in [4,5,6,9]:\n",
    "        return 22\n",
    "    elif num in [11, 12, 13, 14]:\n",
    "        return 24\n",
    "    elif num in [15, 16, 19]:\n",
    "        return 25\n",
    "    elif num == 99: #unknown\n",
    "        return -1\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"Person Type\"\n",
    "code_manual[translate_column_names['ptype']][1] = 'Driver'\n",
    "code_manual[translate_column_names['ptype']][2] = 'Passenger'\n",
    "code_manual[translate_column_names['ptype']][3] = 'Pedestrian/Cyclist'\n",
    "def Collaps_ptype(num):\n",
    "    if num in [-1,19]: #other or not known\n",
    "        return -1\n",
    "    elif num in [2,3,4]:\n",
    "        return 2\n",
    "    elif num in [5,6,7,8,9,10]:\n",
    "        return 3\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"alcres\"  \n",
    "def Collaps_alcres(num):\n",
    "    if num in [-1,95,96,97,98,99]: #other or not known\n",
    "        return -1\n",
    "    elif num in [0]:\n",
    "        return 0\n",
    "    elif num in [1,2,3,4,5]:\n",
    "        return 5\n",
    "    elif num in [6,7,8]:\n",
    "        return 8\n",
    "    return 9\n",
    "\n",
    "# Map Mon-Thursday to one bucket, Fri-Sun to another\n",
    "code_manual[translate_column_names['dayofweek']][10] = 'Mon-Thu'\n",
    "code_manual[translate_column_names['dayofweek']][11] = 'Sat-Sun'\n",
    "def Collaps_dayofweek(num):\n",
    "    #if num in [2,3,4,5]:\n",
    "    #    return 10\n",
    "    return num\n",
    "\n",
    "\n",
    "# Reduce categories in \"EMS Arrival time\"\n",
    "def Collaps_arrtime(num):\n",
    "    if num >= 8800: # unknown\n",
    "        return -1\n",
    "    return num      \n",
    "\n",
    "# Reduce categories in \"Age\"\n",
    "translate_column_names['agegr'] = \"Age group\"\n",
    "def Create_agegr_var(num):\n",
    "    if num>=0 and num <20 :\n",
    "        return 1\n",
    "    if num>=20 and num <40 :\n",
    "        return 2\n",
    "    if num>=40 and num <60 :\n",
    "        return 3\n",
    "    if num>=60:\n",
    "        return 4\n",
    "    return num // 20\n",
    "\n",
    "translate_column_names['killed'] = \"Person died from accident\"\n",
    "# Reduce categories in \"Death date\"\n",
    "def Create_fatality_var(num):\n",
    "    if num >= 88888888: # survived\n",
    "        return 0\n",
    "    return 1      # died\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply reduction of categories cardinality\n",
    "def Collaps_categoricals(data):\n",
    "    if 'lightcond' in data.keys():\n",
    "        data.lightcond = data.lightcond.apply(Collaps_lightcond )\n",
    "    if 'rfun' in data.keys():\n",
    "        data.rfun      = data.rfun.apply(Collaps_rfun  )\n",
    "    if 'holiday' in data.keys():\n",
    "        data.holiday   = data.holiday.apply(Collaps_holiday )\n",
    "    if 'atmncond' in data.keys():\n",
    "        data.atmcond   = data.atmcond.apply(Collaps_atmcond )\n",
    "    if 'ptype' in data.keys():\n",
    "        data.ptype     = data.ptype.apply(Collaps_ptype )\n",
    "    if 'hispanic' in data.keys():\n",
    "        data.hispanic  = data.hispanic.apply(Collaps_hispanic )\n",
    "    if 'race' in data.keys():\n",
    "        data.race      = data.race.apply(Collaps_race)\n",
    "    if 'sex' in data.keys():\n",
    "        data.sex       = data.sex.apply(Collaps_sex)\n",
    "    if 'dayofweek' in data.keys():\n",
    "        data.dayofweek = data.dayofweek.apply(Collaps_dayofweek)\n",
    "    if 'arrtime' in data.keys():\n",
    "        data.arrtime   = data.arrtime.apply(Collaps_arrtime)\n",
    "    if 'alcres' in data.keys():\n",
    "        data.alcres    = data.alcres.apply(Collaps_alcres)\n",
    "    if 'deathdate' in data.keys():\n",
    "        data['killed'] = data.deathdate.apply(Create_fatality_var)\n",
    "        labelsOfInter.append('killed')\n",
    "    if 'age' in data.keys():\n",
    "        data['agegr'] = data.age.apply(Create_agegr_var)\n",
    "        labelsOfInter.append('agegr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map different types of 'unknown' to a single number\n",
    "unknown = {'druginv': [8,9],\n",
    "           'nhs'    : [9],\n",
    "           'reljuncinter': [8,9],\n",
    "           'dayofweek': [-1,9],\n",
    "           'sex'     : [8,9],\n",
    "           'age'     : [-1, 998, 999],\n",
    "           'atmcond' : [8, 0, 98, 99],\n",
    "           'druginv' : [-1, 8, 9],\n",
    "           'race'    : [-1, 0, 98, 99, 97],\n",
    "           'hispanic': [-1,99,0],\n",
    "           'ptype'   : [-1,19],\n",
    "           'rfun'    : [-1, 99],\n",
    "           'lightcond': [-1,7,6,8,9],\n",
    "           'acchr'   : [-1, 88, 99],\n",
    "           'arrhr'   : [-1, 88, 99],\n",
    "           'manncol' : [-1, 98, 99, 11],\n",
    "           'injury'  : [-1, 9]\n",
    "          }\n",
    "\n",
    "# collaps all keys for 'unknown' into '-1' value\n",
    "def Collapse_unknowns(data):\n",
    "    for feature in unknown.keys():\n",
    "        try:\n",
    "            data[feature] = data[feature].apply(lambda x: \n",
    "                                                x if x not in unknown[feature] \n",
    "                                                else -1)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot encoder where needed\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def encode_categoricals(data, target_name):\n",
    "    categorical_features_list = ['driverdrowsy', 'manncol', 'injury', 'ptype', 'druginv', 'heavytruck', \n",
    "                                 'schlbus', 'sex', 'race', 'reljuncinter', 'atmcond', 'holiday', 'nhs', \n",
    "                                 'hispanic', 'rfun', 'lightcond','speeding','dayofweek', 'alcres', 'killed'] \n",
    "    non_categ_features_list = ['arrtime', 'arrhr', 'accdate', 'acchr',\n",
    "                               'deathdate', 'latitude', 'longitude', 'age', 'agegr']\n",
    "    \n",
    "    # Check you know for each feature weather it is categorical or not\n",
    "#    unknowns = set.difference(set(data.keys()),categorical_features_list, non_categ_features_list )\n",
    "#    if len(unknowns)!=0:\n",
    "#        warnings.warn('Are these variables categorical? %s' % unknowns)\n",
    "\n",
    "    # Select categorical features\n",
    "    list_f_cat = list(data.keys() & categorical_features_list)\n",
    "    if target_name in list_f_cat:\n",
    "        list_f_cat.remove(target_name)\n",
    "\n",
    "    # One-hot encode\n",
    "    sdf = data.to_sparse(fill_value=0)\n",
    "    list_dummies = [pd.get_dummies(sdf[feature], prefix=feature, prefix_sep=':').to_sparse(fill_value=0) \n",
    "             for feature in list_f_cat]\n",
    "\n",
    "    # Join it all into one sparse dataframe\n",
    "    sdf = sdf.drop(list_f_cat, 1)\n",
    "    for ii in range(len(list_dummies)):\n",
    "        dummy = list_dummies.pop()\n",
    "        sdf[dummy.keys()]=dummy\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ANOVA to select features\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth_for_classifier = 5\n",
    "rnd_split = 0                     # random_state for train/test split\n",
    "how_many_features_to_consider = 3   # will consider only n-features at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_score_vs_percentile(data, target_name, return_lots_of_things=False, \n",
    "                                percentiles=[5,10, 15, 20, 30, 50],n_estimators=50,\n",
    "                                decisionTree_maxDepth=None,\n",
    "                                with_Adaboost=True):\n",
    "    # on-hot encoder\n",
    "    sdf = encode_categoricals(data, target_name).to_dense()\n",
    "    print('Size of dataset: %.2e bytes'%size_of_dataframe(sdf))\n",
    "    #train-test split\n",
    "    X, y = sdf.drop(target_name,1), sdf[target_name]\n",
    "    train_features, test_feature, train_label, test_label = train_test_split(\n",
    "            X, y, test_size=0.33, random_state=rnd_split)\n",
    "\n",
    "    # run classification \n",
    "    scoring_vs_percent = {}\n",
    "    print('Percentile: ', end='')\n",
    "    for percen_features in percentiles:\n",
    "        print( percen_features, end=',')\n",
    "        selector = SelectPercentile(f_classif, percentile=percen_features)\n",
    "        tr_train_feat = selector.fit_transform(train_features, train_label)\n",
    "        tr_test_feat  = selector.transform(test_feature)  # do not fit on the test, only on train\n",
    "\n",
    "        if with_Adaboost:\n",
    "            clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=decisionTree_maxDepth),\n",
    "                                 n_estimators=n_estimators)\n",
    "        else:\n",
    "            clf = DecisionTreeClassifier(max_depth=decisionTree_maxDepth)\n",
    "        clf.fit(tr_train_feat, train_label)\n",
    "        scoring_vs_percent[percen_features] = clf.score(tr_test_feat, test_label)\n",
    "\n",
    "    # collect results\n",
    "    df = pd.DataFrame({target_name + '_score' : list(scoring_vs_percent.values())}, \n",
    "                      index = list(scoring_vs_percent.keys()))\n",
    "    print('')\n",
    "    if return_lots_of_things:\n",
    "        features_selected = [d for d, s in zip(train_features.keys(), selector.get_support()) if s ]  \n",
    "        test_predict = clf.predict(tr_test_feat)\n",
    "        return df, clf, features_selected, test_label, test_predict\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for exporting Tree-rules\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#label_dictionary = 'Sun Mon Tue Wed Thu Fri Sat'.split()\n",
    "\n",
    "def rules(clf, features, labels, node_index=0, simple_leaf_string=True):\n",
    "    \"\"\"Structure of rules in a fit decision tree classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : DecisionTreeClassifier\n",
    "        A tree that has already been fit.\n",
    "\n",
    "    features, labels : lists of str\n",
    "        The names of the features and labels, respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    if clf.tree_.children_left[node_index] == -1:  # indicates leaf\n",
    "        count_labels = zip(clf.tree_.value[node_index, 0], labels)\n",
    "        node['name'] = ', '.join(('{} of {}'.format(int(count), label)\n",
    "                                  for count, label in count_labels))\n",
    "        if simple_leaf_string:\n",
    "            node['name'] = simplify_string(node['name'], labels)\n",
    "    else:\n",
    "        feature = features[clf.tree_.feature[node_index]]\n",
    "        threshold = clf.tree_.threshold[node_index]\n",
    "        node['name'] = '{} > {}'.format(feature, threshold)\n",
    "        left_index = clf.tree_.children_left[node_index]\n",
    "        right_index = clf.tree_.children_right[node_index]\n",
    "        node['children'] = [rules(clf, features, labels, right_index, simple_leaf_string),\n",
    "                            rules(clf, features, labels, left_index, simple_leaf_string)]\n",
    "    return node\n",
    "\n",
    "def simplify_string(st, labels):\n",
    "    #st = '772 of Sun, 838 of Mon, 862 of Tue, 899 of Wed, 919 of Thu, 1024 of Fri, 994 of Sat'\n",
    "    st = st.split(', ')\n",
    "    nums = [ int(entry.split()[0]) for entry in st ]\n",
    "    pred_day = labels[nums.index(max(nums))]\n",
    "    return pred_day\n",
    "\n",
    "\n",
    "def translate_features_names(features):\n",
    "    '''\n",
    "    After one-hot encoding the names of the features are hardly recognizable\n",
    "    This function looks up the meaning of these names and outputs\n",
    "    a new list with human-friendly names\n",
    "    '''\n",
    "    ylist = []\n",
    "    for name in features:\n",
    "        try:\n",
    "            key, nn = name.split(':')\n",
    "            val = round(float(nn))\n",
    "            out = code_manual[translate_column_names[key]][val]\n",
    "        except ValueError:\n",
    "            key, val = name, None\n",
    "            out = translate_column_names[key]\n",
    "        ylist.append(out)\n",
    "    return(ylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to run\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\Luke-Lenovo\\Desktop\\Database2014.txt ...\n",
      "Number of rows in dataset: 73548\n"
     ]
    }
   ],
   "source": [
    "data = build_dataset()\n",
    "#data.keys()\n",
    "data = data[labelsOfInter].applymap(lambda x: -1 if x == '.' else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after removal of un-knowns: 70549\n"
     ]
    }
   ],
   "source": [
    "# prepare dataset\n",
    "Collaps_categoricals(data)\n",
    "labelsOfInter.remove('deathdate')  # created a dependent variable: 'killed'\n",
    "labelsOfInter.remove('age')        # created a dependent variable: 'agegr'\n",
    "data = data[labelsOfInter]\n",
    "data = Collapse_unknowns(data)\n",
    "\n",
    "keys_for_removal_of_unknowns = []\n",
    "for key in data.keys():\n",
    "    ss = data[key].value_counts().argmin() # select less frequent value for this key\n",
    "    if ss == -1:\n",
    "        keys_for_removal_of_unknowns.append(key)\n",
    "\n",
    "\n",
    "for key in keys_for_removal_of_unknowns:\n",
    "    data = data[data[key] != -1]\n",
    "print('Rows after removal of un-knowns:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "result = pd.DataFrame()\n",
    "n_estimators=50\n",
    "labels_to_predict = ['killed', 'rfun', 'ptype'] #'atmcond','holiday','lightcond','rfun', 'ptype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 killed. Size of dataset: 1.21e+08 bytes\n",
      "Percentile: 5,10,15,20,30,50,\n",
      "2/3 rfun. Size of dataset: 1.15e+08 bytes\n",
      "Percentile: 5,10,15,20,30,50,\n",
      "3/3 ptype. Size of dataset: 1.18e+08 bytes\n",
      "Percentile: 5,10,15,20,30,50,\n"
     ]
    }
   ],
   "source": [
    "# run analysis on all features as target, for different SelectPercentile\n",
    "pickle_filename = \"cest_collapsed_noUnknows_%dRfun_%ddayWeek_2014_3ptype.p\" % (len(set(data.rfun)), len(set(data.dayofweek)) )\n",
    "for target_name in labels_to_predict:\n",
    "    print('%d/%d' % (labels_to_predict.index(target_name)+1, len(labels_to_predict)), target_name, end = '. ')\n",
    "    if result.empty:\n",
    "        result = analyze_score_vs_percentile(data, target_name, n_estimators=n_estimators)\n",
    "    else:\n",
    "        result = result.join( analyze_score_vs_percentile(data, target_name, n_estimators=n_estimators)  )\n",
    "    fname = pickle_filename\n",
    "    if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "    pickle.dump( result, open( fname, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Luke-Lenovo\\\\Dropbox\\\\Python\\\\DataIncubatorChallenge\\\\Q3_ProposeProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make a new directory\n",
    "num = 1\n",
    "path = \".\\\\Trial-%d\" % num\n",
    "while os.path.isdir(path):\n",
    "    path = \".\\\\Trial-%d\" % (num)\n",
    "    num = num+1\n",
    "os.mkdir(path)\n",
    "os.chdir(path)\n",
    "\n",
    "with open(\".\\conditions.txt\", 'w') as f_cond:\n",
    "    print(data.keys(), file=f_cond)\n",
    "    print(\"\", file=f_cond)\n",
    "    for key in data.keys():\n",
    "        print(translate_column_names[key], file=f_cond)\n",
    "    \n",
    "    \n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    killed_score  rfun_score  ptype_score\n",
      "50      0.661049    0.418548     0.676552\n",
      "20      0.662719    0.426992     0.703629\n",
      "5       0.624184    0.392671     0.597173\n",
      "10      0.646402    0.407695     0.684400\n",
      "30      0.667109    0.430862     0.711897\n",
      "15      0.662702    0.411867     0.702782\n",
      "      killed      rfun     ptype\n",
      "50  1.189792  1.154009  1.130641\n",
      "20  1.192798  1.177291  1.175894\n",
      "5   1.123440  1.082660  0.997985\n",
      "10  1.163430  1.124086  1.143757\n",
      "30  1.200699  1.187959  1.189710\n",
      "15  1.192768  1.135588  1.174477\n"
     ]
    }
   ],
   "source": [
    "# normalize score on 'guessing power' (i.e. frequency of most common item)\n",
    "result = pickle.load( open( pickle_filename, \"rb\" ) )\n",
    "print(result)\n",
    "normal_result = pd.DataFrame()\n",
    "for target_name in result.keys():\n",
    "    target_name = target_name.replace('_score', '')\n",
    "    freq = data[target_name].value_counts(normalize = True).iloc[0]\n",
    "    normal_result[target_name] = result[target_name+'_score'].apply(lambda x: x/freq)\n",
    "print(normal_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scatter plot of score and guess_power\n",
    "import bokeh.plotting as bp\n",
    "bp.output_file('Out__Score_vs_GuessingPower.html')\n",
    "colors = ['yellow', 'wheat', 'violet', 'red', 'green',\n",
    "         'sienna', 'seagreen', 'salmon', 'purple', 'PaleTurquoise',\n",
    "         'orange', 'olive', 'navy', 'lime', 'LightSlateGray']\n",
    "\n",
    "score_and_guess = bp.figure(title = 'Adaboost %d estimators, 2012-2014' % n_estimators, \n",
    "                            x_axis_label='Guessing power', \n",
    "                            y_axis_label='Score', y_range=[0,1])\n",
    "\n",
    "colors1 = colors.copy()\n",
    "for target_name in normal_result.keys():\n",
    "    try:\n",
    "        x = normal_result[target_name]\n",
    "        y = result[target_name+'_score']\n",
    "        col = colors1.pop()\n",
    "        s1 = score_and_guess.scatter(x=x,y=y,color=col,size=10,legend=target_name)\n",
    "    except KeyError:\n",
    "        print(target_name, 'not in results: skipping')    \n",
    "        \n",
    "score_and_guess.legend.location = 'bottom_right'\n",
    "score_and_guess.legend.label_text_font_size = '14pt'\n",
    "score_and_guess.xaxis.axis_label_text_font_size = \"22pt\"\n",
    "score_and_guess.xaxis.major_label_text_font_size = '14pt'\n",
    "score_and_guess.yaxis.axis_label_text_font_size = \"22pt\"\n",
    "score_and_guess.yaxis.major_label_text_font_size = '14pt'\n",
    "bp.show(score_and_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp.output_file('Out__GuessingPower_vs_FeaturePercentile.html')\n",
    "\n",
    "guess_and_perc = bp.figure(title = 'Adaboost %d estimators, 2012-2014' % n_estimators, \n",
    "                x_axis_label='Fetures percentile', \n",
    "                y_axis_label='Guessing power')\n",
    "\n",
    "dummy_ = normal_result.reset_index().sort_values(by='index')\n",
    "x = dummy_['index']\n",
    "\n",
    "colors2 = colors.copy()\n",
    "for target_name in normal_result.keys():\n",
    "    try:\n",
    "        y = normal_result[target_name]\n",
    "        col = colors2.pop()\n",
    "        s1 = guess_and_perc.scatter(x=x,y=y,color=col,size=10,legend=target_name)\n",
    "        s2 = guess_and_perc.line(x=x,y=y,color=col,legend=target_name)\n",
    "    except KeyError:\n",
    "        print(target_name, 'not in results: skipping')\n",
    "\n",
    "guess_and_perc.legend.location = 'bottom_right'\n",
    "guess_and_perc.legend.label_text_font_size = '14pt'\n",
    "guess_and_perc.xaxis.axis_label_text_font_size = \"22pt\"\n",
    "guess_and_perc.xaxis.major_label_text_font_size = '14pt'\n",
    "guess_and_perc.yaxis.axis_label_text_font_size = \"22pt\"\n",
    "guess_and_perc.yaxis.major_label_text_font_size = '14pt'\n",
    "bp.show(guess_and_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Selecting one and checking how good predicitons are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_name = 'killed'\n",
    "perc = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print classification report for AdaBoost\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df, clf, features_selected, test_label, test_predict = analyze_score_vs_percentile(data, \n",
    "                                        target_name, return_lots_of_things=True, \n",
    "                                        percentiles=perc)\n",
    "\n",
    "print( classification_report(test_label, test_predict))\n",
    "print (confusion_matrix(test_label, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# score as a function of DecisionTree depth\n",
    "n_estim = 50\n",
    "\n",
    "scores = []\n",
    "depths = [1,3,5,10,20,50, 100]\n",
    "for depth in depths:\n",
    "    print('%s. Depth:%d' % (target_name, depth))\n",
    "    scores.append( analyze_score_vs_percentile(data, target_name, return_lots_of_things=False, \n",
    "                                    percentiles=perc,n_estimators=n_estim,\n",
    "                                    decisionTree_maxDepth=depth).iloc[0] )\n",
    "\n",
    "    \n",
    "bp.output_file('Out__%s_Score_vs_Depth.html'%target_name)\n",
    "\n",
    "score_vs_depth = bp.figure(title = 'Adaboost %d estimators, 2012-2014, %d%% features' % (n_estimators, perc[0]), \n",
    "                x_axis_label='Decision tree depth for %s' % (target_name), \n",
    "                y_axis_label='Score', y_range=[0,1])\n",
    "\n",
    "score_vs_depth.scatter(x=depths, y=scores, color='navy', size=10)\n",
    "bp.show(score_vs_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One tree, print rules \n",
    "\n",
    "1. Select maxdepth=5, 20% features, No Adaboost\n",
    "2. Print rules in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 1.21e+08 bytes\n",
      "Percentile: 30,\n"
     ]
    }
   ],
   "source": [
    "target_name = 'killed'\n",
    "df, clf, features_selected, test_label, test_predict = analyze_score_vs_percentile(data, \n",
    "                                                         target_name, \n",
    "                                                         return_lots_of_things=True, \n",
    "                                                         percentiles=[30],\n",
    "                                                         decisionTree_maxDepth=5,\n",
    "                                                         with_Adaboost=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.78      0.72     66187\n",
      "        1.0       0.66      0.51      0.58     52951\n",
      "\n",
      "avg / total       0.66      0.66      0.66    119138\n",
      "\n",
      "[[51924 14263]\n",
      " [25755 27196]]\n"
     ]
    }
   ],
   "source": [
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print( classification_report(test_label, test_predict))\n",
    "print (confusion_matrix(test_label, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sel_keys =  translate_features_names(features_selected)        # list of keys in the 20% percentile\n",
    "labels = [ code_manual[translate_column_names[target_name]][value] \n",
    "          for value in sorted(list(set(test_label))) ] # list of labels for category\n",
    "r = rules(clf, sel_keys, labels, simple_leaf_string=True)\n",
    "fname = '.\\\\ShowcaseApp\\\\static\\\\structure.json'\n",
    "if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "with open(fname, 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief parethesis: checking for biases in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.ptype.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'ptype'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'agegr'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'rfun'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'sex'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'lightcond'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'rfun'\n",
    "data[tt].plot.hist()\n",
    "data[(data.ptype==3) & (data.killed>0)][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'acchr'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'speeding'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.killed>0].age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.alcres<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.killed>0].alcres.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tar = 'rfun'\n",
    "data[tar].hist(bins=max(data[tar])+2)\n",
    "print('Level1: ', 1./len(set(data[tar])))\n",
    "print('Level2: ', data[tar].value_counts().iloc[0]/len(data) )\n",
    "print('Level3: class. score')\n",
    "\n",
    "#proxy_predict =  26* np.ones(test_label.shape)\n",
    "#print( classification_report(test_label, proxy_predict))\n",
    "#print (confusion_matrix(test_label, proxy_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old strategy: select n features and analyze with those only\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- find best set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def run_analysis(clean_dataset, target_name, returnClassifier=False):\n",
    "    # Separate into train_features, train_label and test_feature, test_label\n",
    "    \n",
    "    X, y = clean_dataset.drop(target_name,1), clean_dataset[target_name]\n",
    "    \n",
    "    train_features, test_feature, train_label, test_label = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=rnd_split)\n",
    "    \n",
    "    # Train decision tree\n",
    "    clf = AdaBoostClassifier(n_estimators=300,\n",
    "                             base_estimator = DecisionTreeClassifier(max_depth = max_depth_for_classifier)\n",
    "                             )\n",
    "    clf.fit(train_features, train_label)\n",
    "    y_predicted = clf.predict(test_feature)\n",
    "     \n",
    "    if returnClassifier:\n",
    "        return clf.score(test_feature, test_label), clf, test_label, y_predicted\n",
    "    return clf.score(test_feature, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations as comb\n",
    "how_many_features_to_consider = 2   # will consider only n-features at once\n",
    "output = {}\n",
    "target_name = 'dayofweek'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ww = ''   # dummy variables, just for printing out the progress of the analysis\n",
    "for sel_feat in comb(labelsOfInter, how_many_features_to_consider):\n",
    "    features = list(sel_feat)\n",
    "   \n",
    "    if target_name in features:\n",
    "        features = features.remove(target_name)\n",
    "        continue\n",
    "    if str(features) in output.keys():\n",
    "        continue\n",
    "        \n",
    "    #is it a new word?\n",
    "    if features[0] != ww:\n",
    "        print('\\n-- Selecting', features[0], ' and')\n",
    "        ww = features[0]\n",
    "    print(features[1:], end=\", \")\n",
    "    \n",
    "    # There are many keys in the dataset for one feature\n",
    "    # because we passed the one-hot encoder.\n",
    "    #  we want to select all keys corresponding to\n",
    "    #  the given feature\n",
    "    sel_keys = []\n",
    "    for feat in features:\n",
    "        for key in data.keys():\n",
    "            if key.startswith(feat):\n",
    "                sel_keys.append(key)\n",
    "    sel_keys.append(target_name)\n",
    "    clean = data[sel_keys]#.copy()\n",
    "    #clean = clean.to_sparse()\n",
    "\n",
    "    output[str(features)] = run_analysis(clean, target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = pd.Series(output)\n",
    "pp = pp.sort_values(ascending=False)[:10]\n",
    "\n",
    "\n",
    "#plot the top 10\n",
    "pl = pp.to_frame('score')\n",
    "pl = pl.reset_index()\n",
    "pl.columns = ['feat', 'score']\n",
    "pl['rel_score'] = pl.score.apply(lambda x: x/max(pl.score)) \n",
    "#pickle.dump( pl, open( \"Output-supp%d.p\" % size_of_dataframe(sdf), \"wb\" ) )\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot top results\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:C:\\Users\\Luke-Lenovo\\Anaconda3\\lib\\site-packages\\bokeh\\core\\validation\\check.py:W-1001 (NO_DATA_RENDERERS): Plot has no data renderers: Figure, ViewModel:Plot, ref _id: d575d8e5-d7a0-4799-a14f-af7b80c80cd6\n",
      "ERROR:C:\\Users\\Luke-Lenovo\\Anaconda3\\lib\\site-packages\\bokeh\\core\\validation\\check.py:W-1001 (NO_DATA_RENDERERS): Plot has no data renderers: Figure, ViewModel:Plot, ref _id: c274b8fd-f22b-4d42-8aaa-090d6051175c\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file, vplot\n",
    "\n",
    "p1 = figure(title=\"Histo\")\n",
    "\n",
    "\n",
    "hist, edges = np.histogram(data.ptype)\n",
    "p1.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "        fill_color=\"#036564\", line_color=\"#033649\")\n",
    "\n",
    "output_file('histogram.html')\n",
    "show(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file, vplot\n",
    "\n",
    "p1 = figure(title=\"Histo\",tools=\"save\",\n",
    "            background_fill_color=\"#E8DDCB\")\n",
    "\n",
    "\n",
    "hist, edges = np.histogram(measured, density=True, bins=50)\n",
    "\n",
    "x = np.linspace(-2, 2, 1000)\n",
    "pdf = 1/(sigma * np.sqrt(2*np.pi)) * np.exp(-(x-mu)**2 / (2*sigma**2))\n",
    "cdf = (1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2\n",
    "\n",
    "p1.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
    "        fill_color=\"#036564\", line_color=\"#033649\")\n",
    "p1.line(x, pdf, line_color=\"#D95B43\", line_width=8, alpha=0.7, legend=\"PDF\")\n",
    "p1.line(x, cdf, line_color=\"white\", line_width=2, alpha=0.7, legend=\"CDF\")\n",
    "\n",
    "p1.legend.location = \"top_left\"\n",
    "p1.xaxis.axis_label = 'x'\n",
    "p1.yaxis.axis_label = 'Pr(x)'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------zz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import Bar\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.charts.attributes import CatAttr\n",
    "from bokeh.plotting import show, output_file\n",
    "#output_notebook()\n",
    "output_file(\"barchart.html\", title=\"results\")\n",
    "\n",
    "#TOOLS = \"pan,wheel_zoom,box_zoom,reset,resize,save\"\n",
    "#fig = figure(tools=TOOLS, toolbar_location=\"left\", logo=\"grey\", plot_width=120)\n",
    "\n",
    "\n",
    "p = Bar(pl, label=CatAttr(columns=['feat'], sort=False),\n",
    "        values='rel_score', \n",
    "        title=\"Scores per set of features\",\n",
    "        color=\"blue\",\n",
    "        ylabel=\"score (relative to max score)\",\n",
    "        xlabel='',\n",
    "        plot_width = 900)\n",
    "p.y_range = Range1d(round(pl.rel_score.iloc[-1],3),pl.rel_score.iloc[1])\n",
    "show(p)\n",
    "\n",
    "\n",
    "#Bar(pl).io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the tree\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the tree for the winning set of features\n",
    "#-------------\n",
    "\n",
    "# retrieve winning set of features\n",
    "ss = pl.feat.iloc[0]\n",
    "for char in ['[', ']', \"'\"]:\n",
    "    ss = ss.replace(char, '')\n",
    "ff = ss.split(',')\n",
    "\n",
    "sel_keys = []\n",
    "for feat in ff:\n",
    "    feat = feat.strip()\n",
    "    for key in data.keys():\n",
    "        if key.startswith(feat):\n",
    "            sel_keys.append(key)\n",
    "sel_keys.append(target_name)\n",
    "\n",
    "# re-do analysis and catch classifier\n",
    "clean = data[sel_keys]\n",
    "num_, classfr, y_true, y_predicted = run_analysis(clean, target_name, returnClassifier=True)\n",
    "\n",
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print( classification_report(y_true, y_predicted, target_names=label_dictionary)  )\n",
    "print (confusion_matrix(y_true, y_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if target_name in sel_keys:\n",
    "    sel_keys.remove(target_name)\n",
    "\n",
    "sel_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = rules(classfr, sel_keys, label_dictionary, simple_leaf_string=True)\n",
    "with open('rules_maxdepth%d.json'%(max_depth_for_classifier), 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
