{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and modules\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\Luke-Lenovo\\Desktop\\Database2014.txt ...\n",
      "Number of rows in dataset: 73548\n"
     ]
    }
   ],
   "source": [
    "FARS_data_files = {2014 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2014.txt',\n",
    "                   #2013 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2013.txt',\n",
    "                   #2012 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2012.txt',\n",
    "                   #2011 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2011.txt',\n",
    "                   #2010 : 'C:\\\\Users\\\\Luke-Lenovo\\\\Desktop\\\\Database2010.txt'\n",
    "                  }\n",
    "\n",
    "def size_of_dataframe(df):\n",
    "    if type(df)==pd.core.series.Series or type(df)==pd.sparse.series.SparseSeries:\n",
    "            return (df.values.nbytes + df.index.nbytes)\n",
    "    return (df.values.nbytes + df.index.nbytes + df.columns.nbytes)\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "# ------------------\n",
    "\n",
    "# Check if files exists\n",
    "def load_file(file_):\n",
    "    print('Loading', file_, '...')\n",
    "    try:\n",
    "        df = pd.read_csv(file_, engine = 'python', sep='\\t', index_col='Obs.')\n",
    "    except OSError:\n",
    "        print('!!file not found!!', file_)\n",
    "        return np.nan\n",
    "    return df\n",
    "\n",
    "def build_dataset():\n",
    "    FARS_dataframes = {}\n",
    "    for year in FARS_data_files.keys():\n",
    "        FARS_dataframes[year] = load_file(FARS_data_files[year])\n",
    "\n",
    "    # Put them all in one dataframe\n",
    "    data = pd.concat(FARS_dataframes)#, ignore_index = True)\n",
    "\n",
    "    # Eliminate the 'Unnamed' column we got from the reading of the csv\n",
    "    for key in data.keys():\n",
    "        if key.startswith('Unnamed'):\n",
    "            data = data.drop(key, 1)\n",
    "    print('Number of rows in dataset:',len(data))\n",
    "    return data\n",
    "\n",
    "# Make it!!\n",
    "data = build_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom-categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib, os\n",
    "os.chdir(\"C:\\\\Users\\\\Luke-Lenovo\\\\Dropbox\\\\Python\\\\DataIncubatorChallenge\\\\Q3_ProposeProject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('.\\\\tt1')\n",
    "import categorization\n",
    "importlib.reload(categorization)\n",
    "code_manual, translate_column_names, labelsOfInter, data = categorization.select_features_and_reduce_cardinality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dayofweek', 'lightcond', 'sex', 'acchr', 'druginv', 'rfun', 'accdate',\n",
       "       'ptype', 'atmcond', 'speeding', 'arrhr', 'killed', 'agegr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot encoder where needed\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "def encode_categoricals(data, target_name):\n",
    "    categorical_features_list = ['driverdrowsy', 'manncol', 'injury', 'ptype', 'druginv', 'heavytruck', \n",
    "                                 'schlbus', 'sex', 'race', 'reljuncinter', 'atmcond', 'holiday', 'nhs', \n",
    "                                 'hispanic', 'rfun', 'lightcond','speeding','dayofweek', 'alcres', 'killed'] \n",
    "    non_categ_features_list = ['arrtime', 'arrhr', 'accdate', 'acchr',\n",
    "                               'deathdate', 'latitude', 'longitude', 'age', 'agegr']\n",
    "    \n",
    "    # Check you know for each feature weather it is categorical or not\n",
    "#    unknowns = set.difference(set(data.keys()),categorical_features_list, non_categ_features_list )\n",
    "#    if len(unknowns)!=0:\n",
    "#        warnings.warn('Are these variables categorical? %s' % unknowns)\n",
    "\n",
    "    # Select categorical features\n",
    "    list_f_cat = list(data.keys() & categorical_features_list)\n",
    "    if target_name in list_f_cat:\n",
    "        list_f_cat.remove(target_name)\n",
    "\n",
    "    # One-hot encode\n",
    "    sdf = data.to_sparse(fill_value=0)\n",
    "    list_dummies = [pd.get_dummies(sdf[feature], prefix=feature, prefix_sep=':').to_sparse(fill_value=0) \n",
    "             for feature in list_f_cat]\n",
    "\n",
    "    # Join it all into one sparse dataframe\n",
    "    sdf = sdf.drop(list_f_cat, 1)\n",
    "    for ii in range(len(list_dummies)):\n",
    "        dummy = list_dummies.pop()\n",
    "        sdf[dummy.keys()]=dummy\n",
    "    \n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ANOVA to select features\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth_for_classifier = 5\n",
    "rnd_split = 0                     # random_state for train/test split\n",
    "how_many_features_to_consider = 3   # will consider only n-features at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyze_score_vs_percentile(data, target_name, return_lots_of_things=False, \n",
    "                                percentiles=[5,10, 15, 20, 30, 50],n_estimators=50,\n",
    "                                decisionTree_maxDepth=None,\n",
    "                                with_Adaboost=True):\n",
    "    # on-hot encoder\n",
    "    sdf = encode_categoricals(data, target_name).to_dense()\n",
    "    print('Size of dataset: %.2e bytes'%size_of_dataframe(sdf))\n",
    "    #train-test split\n",
    "    X, y = sdf.drop(target_name,1), sdf[target_name]\n",
    "    train_features, test_feature, train_label, test_label = train_test_split(\n",
    "            X, y, test_size=0.33, random_state=rnd_split)\n",
    "\n",
    "    # run classification \n",
    "    scoring_vs_percent = {}\n",
    "    print('Percentile: ', end='')\n",
    "    for percen_features in percentiles:\n",
    "        print( percen_features, end=',')\n",
    "        selector = SelectPercentile(f_classif, percentile=percen_features)\n",
    "        tr_train_feat = selector.fit_transform(train_features, train_label)\n",
    "        tr_test_feat  = selector.transform(test_feature)  # do not fit on the test, only on train\n",
    "\n",
    "        if with_Adaboost:\n",
    "            clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=decisionTree_maxDepth),\n",
    "                                 n_estimators=n_estimators)\n",
    "        else:\n",
    "            clf = DecisionTreeClassifier(max_depth=decisionTree_maxDepth)\n",
    "        clf.fit(tr_train_feat, train_label)\n",
    "        scoring_vs_percent[percen_features] = clf.score(tr_test_feat, test_label)\n",
    "\n",
    "    # collect results\n",
    "    df = pd.DataFrame({target_name + '_score' : list(scoring_vs_percent.values())}, \n",
    "                      index = list(scoring_vs_percent.keys()))\n",
    "    print('')\n",
    "    if return_lots_of_things:\n",
    "        features_selected = [d for d, s in zip(train_features.keys(), selector.get_support()) if s ]  \n",
    "        test_predict = clf.predict(tr_test_feat)\n",
    "        return df, clf, features_selected, test_label, test_predict\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for exporting Tree-rules\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#label_dictionary = 'Sun Mon Tue Wed Thu Fri Sat'.split()\n",
    "\n",
    "def rules(clf, features, labels, node_index=0, simple_leaf_string=True):\n",
    "    \"\"\"Structure of rules in a fit decision tree classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : DecisionTreeClassifier\n",
    "        A tree that has already been fit.\n",
    "\n",
    "    features, labels : lists of str\n",
    "        The names of the features and labels, respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    if clf.tree_.children_left[node_index] == -1:  # indicates leaf\n",
    "        count_labels = zip(clf.tree_.value[node_index, 0], labels)\n",
    "        node['name'] = ', '.join(('{} of {}'.format(int(count), label)\n",
    "                                  for count, label in count_labels))\n",
    "        if simple_leaf_string:\n",
    "            node['name'] = simplify_string(node['name'], labels)\n",
    "    else:\n",
    "        feature = features[clf.tree_.feature[node_index]]\n",
    "        threshold = clf.tree_.threshold[node_index]\n",
    "        node['name'] = '{} > {}'.format(feature, threshold)\n",
    "        left_index = clf.tree_.children_left[node_index]\n",
    "        right_index = clf.tree_.children_right[node_index]\n",
    "        node['children'] = [rules(clf, features, labels, right_index, simple_leaf_string),\n",
    "                            rules(clf, features, labels, left_index, simple_leaf_string)]\n",
    "    return node\n",
    "\n",
    "def simplify_string(st, labels):\n",
    "    #st = '772 of Sun, 838 of Mon, 862 of Tue, 899 of Wed, 919 of Thu, 1024 of Fri, 994 of Sat'\n",
    "    st = st.split(', ')\n",
    "    nums = [ int(entry.split()[0]) for entry in st ]\n",
    "    pred_day = labels[nums.index(max(nums))]\n",
    "    return pred_day\n",
    "\n",
    "\n",
    "def translate_features_names(features):\n",
    "    '''\n",
    "    After one-hot encoding the names of the features are hardly recognizable\n",
    "    This function looks up the meaning of these names and outputs\n",
    "    a new list with human-friendly names\n",
    "    '''\n",
    "    ylist = []\n",
    "    for name in features:\n",
    "        try:\n",
    "            key, nn = name.split(':')\n",
    "            val = round(float(nn))\n",
    "            out = code_manual[translate_column_names[key]][val]\n",
    "        except ValueError:\n",
    "            key, val = name, None\n",
    "            out = translate_column_names[key]\n",
    "        ylist.append(out)\n",
    "    return(ylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to run\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys_for_removal_of_unknowns = []\n",
    "for key in data.keys():\n",
    "    ss = data[key].value_counts().argmin() # select less frequent value for this key\n",
    "    if ss == -1:\n",
    "        keys_for_removal_of_unknowns.append(key)\n",
    "\n",
    "\n",
    "for key in keys_for_removal_of_unknowns:\n",
    "    data = data[data[key] != -1]\n",
    "print('Rows after removal of un-knowns:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "result = pd.DataFrame()\n",
    "n_estimators=50\n",
    "labels_to_predict = ['killed', 'rfun', 'ptype'] #'atmcond','holiday','lightcond','rfun', 'ptype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run analysis on all features as target, for different SelectPercentile\n",
    "pickle_filename = \"cest_collapsed_noUnknows_%dRfun_%ddayWeek_2014_3ptype.p\" % (len(set(data.rfun)), len(set(data.dayofweek)) )\n",
    "for target_name in labels_to_predict:\n",
    "    print('%d/%d' % (labels_to_predict.index(target_name)+1, len(labels_to_predict)), target_name, end = '. ')\n",
    "    if result.empty:\n",
    "        result = analyze_score_vs_percentile(data, target_name, n_estimators=n_estimators)\n",
    "    else:\n",
    "        result = result.join( analyze_score_vs_percentile(data, target_name, n_estimators=n_estimators)  )\n",
    "    fname = pickle_filename\n",
    "    if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "    pickle.dump( result, open( fname, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize score on 'guessing power' (i.e. frequency of most common item)\n",
    "result = pickle.load( open( pickle_filename, \"rb\" ) )\n",
    "print(result)\n",
    "normal_result = pd.DataFrame()\n",
    "for target_name in result.keys():\n",
    "    target_name = target_name.replace('_score', '')\n",
    "    freq = data[target_name].value_counts(normalize = True).iloc[0]\n",
    "    normal_result[target_name] = result[target_name+'_score'].apply(lambda x: x/freq)\n",
    "print(normal_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scatter plot of score and guess_power\n",
    "import bokeh.plotting as bp\n",
    "bp.output_file('Out__Score_vs_GuessingPower.html')\n",
    "colors = ['yellow', 'wheat', 'violet', 'red', 'green',\n",
    "         'sienna', 'seagreen', 'salmon', 'purple', 'PaleTurquoise',\n",
    "         'orange', 'olive', 'navy', 'lime', 'LightSlateGray']\n",
    "\n",
    "score_and_guess = bp.figure(title = 'Adaboost %d estimators, 2012-2014' % n_estimators, \n",
    "                            x_axis_label='Guessing power', \n",
    "                            y_axis_label='Score', y_range=[0,1])\n",
    "\n",
    "colors1 = colors.copy()\n",
    "for target_name in normal_result.keys():\n",
    "    try:\n",
    "        x = normal_result[target_name]\n",
    "        y = result[target_name+'_score']\n",
    "        col = colors1.pop()\n",
    "        s1 = score_and_guess.scatter(x=x,y=y,color=col,size=10,legend=target_name)\n",
    "    except KeyError:\n",
    "        print(target_name, 'not in results: skipping')    \n",
    "        \n",
    "score_and_guess.legend.location = 'bottom_right'\n",
    "score_and_guess.legend.label_text_font_size = '14pt'\n",
    "score_and_guess.xaxis.axis_label_text_font_size = \"22pt\"\n",
    "score_and_guess.xaxis.major_label_text_font_size = '14pt'\n",
    "score_and_guess.yaxis.axis_label_text_font_size = \"22pt\"\n",
    "score_and_guess.yaxis.major_label_text_font_size = '14pt'\n",
    "bp.show(score_and_guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bp.output_file('Out__GuessingPower_vs_FeaturePercentile.html')\n",
    "\n",
    "guess_and_perc = bp.figure(title = 'Adaboost %d estimators, 2012-2014' % n_estimators, \n",
    "                x_axis_label='Fetures percentile', \n",
    "                y_axis_label='Guessing power')\n",
    "\n",
    "dummy_ = normal_result.reset_index().sort_values(by='index')\n",
    "x = dummy_['index']\n",
    "\n",
    "colors2 = colors.copy()\n",
    "for target_name in normal_result.keys():\n",
    "    try:\n",
    "        y = normal_result[target_name]\n",
    "        col = colors2.pop()\n",
    "        s1 = guess_and_perc.scatter(x=x,y=y,color=col,size=10,legend=target_name)\n",
    "        s2 = guess_and_perc.line(x=x,y=y,color=col,legend=target_name)\n",
    "    except KeyError:\n",
    "        print(target_name, 'not in results: skipping')\n",
    "\n",
    "guess_and_perc.legend.location = 'bottom_right'\n",
    "guess_and_perc.legend.label_text_font_size = '14pt'\n",
    "guess_and_perc.xaxis.axis_label_text_font_size = \"22pt\"\n",
    "guess_and_perc.xaxis.major_label_text_font_size = '14pt'\n",
    "guess_and_perc.yaxis.axis_label_text_font_size = \"22pt\"\n",
    "guess_and_perc.yaxis.major_label_text_font_size = '14pt'\n",
    "bp.show(guess_and_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Selecting one and checking how good predicitons are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_name = 'killed'\n",
    "perc = [20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print classification report for AdaBoost\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df, clf, features_selected, test_label, test_predict = analyze_score_vs_percentile(data, \n",
    "                                        target_name, return_lots_of_things=True, \n",
    "                                        percentiles=perc)\n",
    "\n",
    "print( classification_report(test_label, test_predict))\n",
    "print (confusion_matrix(test_label, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# score as a function of DecisionTree depth\n",
    "n_estim = 50\n",
    "\n",
    "scores = []\n",
    "depths = [1,3,5,10,20,50, 100]\n",
    "for depth in depths:\n",
    "    print('%s. Depth:%d' % (target_name, depth))\n",
    "    scores.append( analyze_score_vs_percentile(data, target_name, return_lots_of_things=False, \n",
    "                                    percentiles=perc,n_estimators=n_estim,\n",
    "                                    decisionTree_maxDepth=depth).iloc[0] )\n",
    "\n",
    "    \n",
    "bp.output_file('Out__%s_Score_vs_Depth.html'%target_name)\n",
    "\n",
    "score_vs_depth = bp.figure(title = 'Adaboost %d estimators, 2012-2014, %d%% features' % (n_estimators, perc[0]), \n",
    "                x_axis_label='Decision tree depth for %s' % (target_name), \n",
    "                y_axis_label='Score', y_range=[0,1])\n",
    "\n",
    "score_vs_depth.scatter(x=depths, y=scores, color='navy', size=10)\n",
    "bp.show(score_vs_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One tree, print rules \n",
    "\n",
    "1. Select maxdepth=5, 20% features, No Adaboost\n",
    "2. Print rules in JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_name = 'killed'\n",
    "df, clf, features_selected, test_label, test_predict = analyze_score_vs_percentile(data, \n",
    "                                                         target_name, \n",
    "                                                         return_lots_of_things=True, \n",
    "                                                         percentiles=[30],\n",
    "                                                         decisionTree_maxDepth=5,\n",
    "                                                         with_Adaboost=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print( classification_report(test_label, test_predict))\n",
    "print (confusion_matrix(test_label, test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sel_keys =  translate_features_names(features_selected)        # list of keys in the 20% percentile\n",
    "labels = [ code_manual[translate_column_names[target_name]][value] \n",
    "          for value in sorted(list(set(test_label))) ] # list of labels for category\n",
    "r = rules(clf, sel_keys, labels, simple_leaf_string=True)\n",
    "fname = '.\\\\ShowcaseApp\\\\static\\\\structure.json'\n",
    "if os.path.isfile(fname):\n",
    "        os.remove(fname)\n",
    "with open(fname, 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief parethesis: checking for biases in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.ptype.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'ptype'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'agegr'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'rfun'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'sex'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'lightcond'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'rfun'\n",
    "data[tt].plot.hist()\n",
    "data[(data.ptype==3) & (data.killed>0)][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'acchr'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = 'speeding'\n",
    "data[tt].plot.hist()\n",
    "data[data.killed>0][tt].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.killed>0].age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.alcres<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[data.killed>0].alcres.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tar = 'rfun'\n",
    "data[tar].hist(bins=max(data[tar])+2)\n",
    "print('Level1: ', 1./len(set(data[tar])))\n",
    "print('Level2: ', data[tar].value_counts().iloc[0]/len(data) )\n",
    "print('Level3: class. score')\n",
    "\n",
    "#proxy_predict =  26* np.ones(test_label.shape)\n",
    "#print( classification_report(test_label, proxy_predict))\n",
    "#print (confusion_matrix(test_label, proxy_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old strategy: select n features and analyze with those only\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- find best set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def run_analysis(clean_dataset, target_name, returnClassifier=False):\n",
    "    # Separate into train_features, train_label and test_feature, test_label\n",
    "    \n",
    "    X, y = clean_dataset.drop(target_name,1), clean_dataset[target_name]\n",
    "    \n",
    "    train_features, test_feature, train_label, test_label = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=rnd_split)\n",
    "    \n",
    "    # Train decision tree\n",
    "    clf = AdaBoostClassifier(n_estimators=300,\n",
    "                             base_estimator = DecisionTreeClassifier(max_depth = max_depth_for_classifier)\n",
    "                             )\n",
    "    clf.fit(train_features, train_label)\n",
    "    y_predicted = clf.predict(test_feature)\n",
    "     \n",
    "    if returnClassifier:\n",
    "        return clf.score(test_feature, test_label), clf, test_label, y_predicted\n",
    "    return clf.score(test_feature, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations as comb\n",
    "how_many_features_to_consider = 2   # will consider only n-features at once\n",
    "output = {}\n",
    "target_name = 'dayofweek'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ww = ''   # dummy variables, just for printing out the progress of the analysis\n",
    "for sel_feat in comb(labelsOfInter, how_many_features_to_consider):\n",
    "    features = list(sel_feat)\n",
    "   \n",
    "    if target_name in features:\n",
    "        features = features.remove(target_name)\n",
    "        continue\n",
    "    if str(features) in output.keys():\n",
    "        continue\n",
    "        \n",
    "    #is it a new word?\n",
    "    if features[0] != ww:\n",
    "        print('\\n-- Selecting', features[0], ' and')\n",
    "        ww = features[0]\n",
    "    print(features[1:], end=\", \")\n",
    "    \n",
    "    # There are many keys in the dataset for one feature\n",
    "    # because we passed the one-hot encoder.\n",
    "    #  we want to select all keys corresponding to\n",
    "    #  the given feature\n",
    "    sel_keys = []\n",
    "    for feat in features:\n",
    "        for key in data.keys():\n",
    "            if key.startswith(feat):\n",
    "                sel_keys.append(key)\n",
    "    sel_keys.append(target_name)\n",
    "    clean = data[sel_keys]#.copy()\n",
    "    #clean = clean.to_sparse()\n",
    "\n",
    "    output[str(features)] = run_analysis(clean, target_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = pd.Series(output)\n",
    "pp = pp.sort_values(ascending=False)[:10]\n",
    "\n",
    "\n",
    "#plot the top 10\n",
    "pl = pp.to_frame('score')\n",
    "pl = pl.reset_index()\n",
    "pl.columns = ['feat', 'score']\n",
    "pl['rel_score'] = pl.score.apply(lambda x: x/max(pl.score)) \n",
    "#pickle.dump( pl, open( \"Output-supp%d.p\" % size_of_dataframe(sdf), \"wb\" ) )\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot top results\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import Bar\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.charts.attributes import CatAttr\n",
    "from bokeh.plotting import show, output_file\n",
    "#output_notebook()\n",
    "output_file(\"barchart.html\", title=\"results\")\n",
    "\n",
    "#TOOLS = \"pan,wheel_zoom,box_zoom,reset,resize,save\"\n",
    "#fig = figure(tools=TOOLS, toolbar_location=\"left\", logo=\"grey\", plot_width=120)\n",
    "\n",
    "\n",
    "p = Bar(pl, label=CatAttr(columns=['feat'], sort=False),\n",
    "        values='rel_score', \n",
    "        title=\"Scores per set of features\",\n",
    "        color=\"blue\",\n",
    "        ylabel=\"score (relative to max score)\",\n",
    "        xlabel='',\n",
    "        plot_width = 900)\n",
    "p.y_range = Range1d(round(pl.rel_score.iloc[-1],3),pl.rel_score.iloc[1])\n",
    "show(p)\n",
    "\n",
    "\n",
    "#Bar(pl).io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the tree\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the tree for the winning set of features\n",
    "#-------------\n",
    "\n",
    "# retrieve winning set of features\n",
    "ss = pl.feat.iloc[0]\n",
    "for char in ['[', ']', \"'\"]:\n",
    "    ss = ss.replace(char, '')\n",
    "ff = ss.split(',')\n",
    "\n",
    "sel_keys = []\n",
    "for feat in ff:\n",
    "    feat = feat.strip()\n",
    "    for key in data.keys():\n",
    "        if key.startswith(feat):\n",
    "            sel_keys.append(key)\n",
    "sel_keys.append(target_name)\n",
    "\n",
    "# re-do analysis and catch classifier\n",
    "clean = data[sel_keys]\n",
    "num_, classfr, y_true, y_predicted = run_analysis(clean, target_name, returnClassifier=True)\n",
    "\n",
    "# print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print( classification_report(y_true, y_predicted, target_names=label_dictionary)  )\n",
    "print (confusion_matrix(y_true, y_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if target_name in sel_keys:\n",
    "    sel_keys.remove(target_name)\n",
    "\n",
    "sel_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = rules(classfr, sel_keys, label_dictionary, simple_leaf_string=True)\n",
    "with open('rules_maxdepth%d.json'%(max_depth_for_classifier), 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
